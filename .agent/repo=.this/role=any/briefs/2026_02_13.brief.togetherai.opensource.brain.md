# Open-Source Frontier Model Fine-Tuning & Agentic Capability Brief

**Date:** 2026-02-13
**Focus:** Together AI, open-source frontier models, fine-tuning landscape, agentic tool use

---

## The Problem

Fine-tuning frontier LLMs on major cloud providers is limited:

- **AWS Bedrock** — Only Llama 3.x (up to 90B), Claude 3 Haiku, and Amazon Nova. No DeepSeek, Mistral, or top-tier proprietary models.
- **Azure AI Foundry** — Mistral Large/Nemo/Ministral only.
- **OpenAI** — GPT-4o, GPT-4.1 nano, gpt-oss. No Codex or GPT-5 fine-tuning.
- **xAI** — No fine-tuning API for Grok at all.

None of the hyperscalers offer fine-tuning of the largest open-source frontier models (671B+ DeepSeek, 1T Kimi K2, 480B Qwen3-Coder).

---

## Together AI: The Open-Source Fine-Tuning Platform

**What:** Cloud platform purpose-built for open-source AI — inference, fine-tuning, training, GPU clusters.
**Philosophy:** AI should be open. No vendor lock-in. Customers own their models.
**Scale:** $305M Series B (Feb 2025), ~$300M ARR, data centers in Maryland, Memphis, Sweden.

### Frontier Models Available for Fine-Tuning

| Model | Params | Active | Fine-Tune Method | SFT Cost/1M tokens | Min Charge |
|---|---|---|---|---|---|
| Kimi K2 (Moonshot) | 1T | — | LoRA | $15.00 | $60 |
| DeepSeek V3.1 / R1 | 671B | 37B | LoRA | $10.00 | $20 |
| Qwen3-Coder 480B | 480B | 35B | LoRA | $6.00 | None |
| Qwen3 235B | 235B | 22B | LoRA | $6.00 | None |
| OpenAI gpt-oss 120B | 120B | — | LoRA + Full | Standard tier | — |
| Llama 4 Maverick | MoE | — | LoRA | $8.00 | $16 |
| Llama 3.3 70B | 70B | 70B | LoRA + Full | $2.90 | — |
| GLM-4.7 (Zhipu) | — | — | LoRA + Full | Standard tier | — |

**Fine-tuning methods:** LoRA, Full, DPO, Long-context LoRA (up to 131K tokens).

### Standard Fine-Tuning Pricing (per 1M tokens)

| Size | LoRA (SFT) | Full (SFT) | LoRA (DPO) | Full (DPO) |
|---|---|---|---|---|
| Up to 16B | $0.48 | $0.54 | $1.20 | $1.35 |
| 17B-69B | $1.50 | $1.65 | $3.75 | $4.12 |
| 70-100B | $2.90 | $3.20 | $7.25 | $8.00 |

---

## Agentic Coding Performance (SWE-Bench Verified, Feb 2026)

### Proprietary Leaders

| Model | Score | Cost (input/output per 1M) |
|---|---|---|
| Claude Sonnet 5 | ~82% | $3 / $15 |
| Claude Opus 4.5 | 80.9% | — |
| Claude Opus 4.6 | 80.8% | — |
| GPT-5.2 | 80.0% | $1.25 / $10 |
| Grok 4 | 72-75% | — |

### Open-Source (Available on Together AI)

> **note:** costs below are together ai serverless rates unless marked otherwise.
> some models are available cheaper via other providers (chutes, parasail, openrouter).
> deepseek v3.2 is not yet live on together ai (status: soon); v3.1 is available.

| Model | Score | Together AI (input/output per 1M) | Cheapest provider (input/output per 1M) |
|---|---|---|---|
| GLM-5 | 77.8% | tbd (just released 2026-02-11) | tbd |
| Kimi K2.5 | 76.8% | $0.50 / $2.80 | — |
| Qwen3-Coder-Next | 74.2% | $0.50 / $1.20 | $0.07 / $0.30 (chutes) |
| GLM-4.7 | 73.8% | $0.45 / $2.00 | — |
| Kimi K2 (thought variant) | 71.3% | $1.20 / $4.00 | — |
| Qwen3-Coder 480B | 69.6% | $2.00 / $2.00 | — |
| DeepSeek V3.1 | — | $0.60 / $1.70 | — |

**Gap:** ~5-10 points between best open-source (77.8% glm-5) and best proprietary (82% claude sonnet 5).

---

## Cost-Performance Sweet Spots

### Qwen3-Coder-Next (Best $/performance on Together AI)

- **3B active params**, 80B total, 262K context
- **$0.50 input / $1.20 output** per 1M tokens (together ai serverless)
- **$0.07 input / $0.30 output** per 1M tokens (chutes / openrouter — cheapest provider)
- 74.2% SWE-Bench — 6x cheaper input than Claude Sonnet 5 (on together ai); 43x cheaper via chutes
- Fine-tunable on Together AI at $6/1M tokens (no minimum)

### GLM-5 (Best open-source accuracy — new)

- 744B total, 40B active (moe), 200K context
- 77.8% SWE-Bench — best open-source score, released 2026-02-11
- Together AI rates: tbd (just released)
- MIT license

### DeepSeek V3.1 (Available) / V3.2 (Soon)

- 671B total, 37B active, 128K context
- **$0.60 input / $1.70 output** per 1M tokens (together ai serverless, v3.1)
- V3.2 (73.0% SWE-Bench) is not yet live on together ai (status: soon); only v3.1 is available
- Fine-tunable on Together AI at $10/1M tokens ($20 minimum)

### Comparison at a Glance (Together AI rates)

| | Qwen3-Coder-Next | DeepSeek V3.1 | Claude Sonnet 5 |
|---|---|---|---|
| SWE-Bench | 74.2% | — (v3.2 = 73%) | ~82% |
| Input $/1M | $0.50 | $0.60 | $3.00 |
| Output $/1M | $1.20 | $1.70 | $15.00 |
| Fine-tunable | Yes | Yes | No |
| Relative cost (input) | 1x | 1.2x | 6x |

### Comparison at a Glance (Cheapest provider rates)

| | Qwen3-Coder-Next (chutes) | DeepSeek V3.2 (deepseek.com) | Claude Sonnet 5 |
|---|---|---|---|
| SWE-Bench | 74.2% | 73.0% | ~82% |
| Input $/1M | $0.07 | $0.28 | $3.00 |
| Output $/1M | $0.30 | $0.42 | $15.00 |
| Fine-tunable | No (inference only) | No (inference only) | No |
| Relative cost (input) | 1x | 4x | 43x |

> **note:** cheapest-provider rates do not include fine-tune access. fine-tune is only available via together ai.

---

## Key Insights

1. **Together AI is the only platform with managed fine-tune of 671B+ frontier open-source models** (DeepSeek, Kimi K2, Qwen3-Coder). Hyperscalers don't touch this.

2. **Open-source is at ~85-95% of proprietary capability** for agentic code (77.8% glm-5 vs 82% claude sonnet 5). The gap has narrowed.

3. **Together AI rates are higher than cheapest-provider rates.** Qwen3-Coder-Next is $0.50/$1.20 on together ai vs $0.07/$0.30 on chutes. The premium buys reliability, throughput, and fine-tune access.

4. **Qwen3-Coder-Next is the efficiency outlier** — 74.2% SWE-Bench from 3B active params. On together ai: 6x cheaper than claude. Via chutes: 43x cheaper.

5. **Fine-tune could close the gap.** The 5-8 point deficit to Claude/Codex on domain-specific tasks may be recoverable with targeted SFT/DPO on task-specific data. Fine-tune is only available via together ai, not via cheap providers.

6. **DeepSeek V3.2 (73% swe-bench) is not yet on together ai.** Only V3.1 is live. When V3.2 lands, it will be a strong contender.

7. **GLM-5 (77.8% swe-bench) just released.** MIT license, 744B/40B moe. Together ai rates tbd. Closest open-source model to proprietary frontier.

8. **No one offers fine-tune of Grok, Codex, Claude, or GPT-5.** The proprietary frontier remains closed.
