# research: together-ai api access

> remote access patterns for rhachet-brains-togetherai

---

## 1. remote repositories

### 1.1 together ai inference api

the primary remote service is the together ai inference api.

| attribute | value |
|-----------|-------|
| base url | `https://api.together.xyz/v1` |
| auth | bearer token via `TOGETHER_API_KEY` |
| protocol | https rest api |
| format | openai-compatible json |

**citation [1]**: "To start using Together with OpenAI's client libraries, pass in your Together API key to the api_key option, and change the base_url to https://api.together.xyz/v1" — [together ai openai compatibility docs](https://docs.together.ai/docs/openai-api-compatibility)

---

## 2. contracts and interfaces

### 2.1 native sdk: `together-ai` npm package

**citation [2]**: "The library provides convenient access to the Together REST API from server-side TypeScript or JavaScript." — [together-typescript github](https://github.com/togethercomputer/together-typescript)

```bash
npm install together-ai
```

**basic usage:**
```typescript
import Together from 'together-ai';

const client = new Together({
  apiKey: process.env['TOGETHER_API_KEY'],
});

const chatCompletion = await client.chat.completions.create({
  messages: [{ role: 'user', content: 'Say this is a test!' }],
  model: 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',
});
```

**citation [3]**: "The library includes TypeScript definitions for all request params and response fields" — [together-typescript github](https://github.com/togethercomputer/together-typescript)

### 2.2 openai sdk compatibility

together ai is fully openai-compatible, so we can use the `openai` npm package with a base url override.

**citation [4]**: "The request structure remains identical to OpenAI's client libraries. Messages use the same role-based format (system, user, assistant), and responses follow OpenAI's standard completion object structure with `choices[0].message.content` for content extraction." — [together ai openai compatibility docs](https://docs.together.ai/docs/openai-api-compatibility)

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.TOGETHER_API_KEY,
  baseURL: 'https://api.together.xyz/v1',
});
```

### 2.3 chat completions endpoint

**endpoint:** `POST /v1/chat/completions`

**request format:**
```typescript
{
  model: string;                    // e.g., 'Qwen/Qwen3-Coder-Next-80B-A3B-Instruct'
  messages: Array<{
    role: 'system' | 'user' | 'assistant';
    content: string;
  }>;
  stream?: boolean;                 // enable SSE
  response_format?: {
    type: 'json_object' | 'json_schema';
    schema?: object;                // json schema for structured output
  };
  tools?: Array<ToolDefinition>;    // function call support
  tool_choice?: 'auto' | 'none' | { type: 'function'; function: { name: string } };
}
```

**response format:**
```typescript
{
  id: string;
  object: 'chat.completion';
  created: number;
  model: string;
  choices: Array<{
    index: number;
    message: {
      role: 'assistant';
      content: string;
    };
    finish_reason: string;
  }>;
  usage: {
    prompt_tokens: number;
    completion_tokens: number;
    total_tokens: number;
  };
}
```

### 2.4 structured output (json schema)

**citation [5]**: "When a schema is passed in, we enforce the model to generate the output aligned with the given schema." — [together ai json mode blog](https://www.together.ai/blog/function-calling-json-mode)

```typescript
const response = await client.chat.completions.create({
  model: 'mistralai/Mixtral-8x7B-Instruct-v0.1',
  messages: [...],
  response_format: {
    type: 'json_object',
    schema: MySchema.model_json_schema(),  // pydantic style, or raw json schema
  },
});
```

**note:** the `json_schema` type with strict mode (as used by openai) may work via compatibility layer, but the native together api uses `json_object` with a `schema` field.

---

## 3. error codes and retry behavior

**citation [6]**: "Errors are subclasses of `APIError`. Status codes map to specific error types" — [together-typescript github](https://github.com/togethercomputer/together-typescript)

| status | error type | retry? |
|--------|------------|--------|
| 400 | `BadRequestError` | no |
| 401 | `AuthenticationError` | no |
| 403 | `PermissionDeniedError` | no |
| 404 | `NotFoundError` | no |
| 429 | `RateLimitError` | yes (backoff) |
| ≥500 | `InternalServerError` | yes |

**citation [7]**: "Default is 2 retries for connection errors, timeouts, and 5xx errors." — [together-typescript github](https://github.com/togethercomputer/together-typescript)

---

## 4. available models — top 10 brain specs

> all costs below are together ai serverless rates, not provider-direct rates.
> provider-direct rates (e.g., deepseek.com, qwen docs) are often cheaper but not relevant here.

### 4.1 model #1: glm-5

| attribute | value |
|-----------|-------|
| model id | `zai-org/GLM-5` |
| params | 744B total, 40B active (moe) |
| context | 200K tokens |
| swe-bench | 77.8% verified |
| input cost | tbd (just released 2026-02-11) |
| output cost | tbd |
| license | mit |

**citation [8]**: "GLM-5 scores 77.8% on SWE-bench Verified" — [zhipu glm-5 release](https://zhipuai.cn/)

**note:** glm-5 was released 2026-02-11 and may not yet have together ai serverless rates. check availability.

### 4.2 model #2: kimi k2.5 instruct

| attribute | value |
|-----------|-------|
| model id | `moonshotai/Kimi-K2.5-Instruct` |
| params | 1T total, 32B active (moe) |
| context | 262K tokens |
| swe-bench | 76.8% verified |
| input cost | $0.50 / 1M tokens |
| output cost | $2.80 / 1M tokens |
| license | proprietary (moonshot) |

**citation [9]**: "Kimi K2.5: Input $0.50 / Output $2.80" — [together ai cost page](https://www.together.ai/pricing)

### 4.3 model #3: qwen3-coder-next (fp8)

| attribute | value |
|-----------|-------|
| model id | `Qwen/Qwen3-Coder-Next-80B-A3B-Instruct-FP8` |
| params | 80B total, 3B active (moe) |
| context | 262K tokens |
| swe-bench | 74.2% verified |
| input cost | $0.50 / 1M tokens |
| output cost | $1.20 / 1M tokens |
| license | apache 2.0 |
| fine-tune | yes, via together ai |

**citation [10]**: "Qwen3-Coder-Next: Input $0.50 / Output $1.20" — [together ai cost page](https://www.together.ai/pricing)

**note:** the brief cites $0.07/$0.30 — those are provider-direct rates from qwen, not together ai rates. the actual together ai rate is $0.50/$1.20.

### 4.4 model #4: glm-4.7

| attribute | value |
|-----------|-------|
| model id | `zai-org/GLM-4.7` |
| params | undisclosed |
| context | 200K tokens |
| swe-bench | 73.8% verified |
| input cost | $0.45 / 1M tokens |
| output cost | $2.00 / 1M tokens |
| license | mit |

**citation [11]**: "GLM-4.7: Input $0.45 / Output $2.00" — [together ai cost page](https://www.together.ai/pricing)

### 4.5 model #5: qwen3-coder-480b

| attribute | value |
|-----------|-------|
| model id | `Qwen/Qwen3-Coder-480B-A35B-Instruct-FP8` |
| params | 480B total, 35B active (moe) |
| context | 256K tokens |
| swe-bench | 69.6% verified |
| input cost | $2.00 / 1M tokens |
| output cost | $2.00 / 1M tokens |
| license | apache 2.0 |
| fine-tune | yes, via together ai ($6/1M tokens) |

**citation [12]**: "Qwen3-Coder-480B: Input $2.00 / Output $2.00" — [together ai cost page](https://www.together.ai/pricing)

### 4.6 model #6: deepseek v3.1

| attribute | value |
|-----------|-------|
| model id | `deepseek-ai/DeepSeek-V3.1` |
| params | 671B total, 37B active (moe) |
| context | 128K tokens |
| swe-bench | — (v3.2 scores 73%, but v3.2 is not yet on together ai) |
| input cost | $0.60 / 1M tokens |
| output cost | $1.70 / 1M tokens |
| license | mit |
| fine-tune | yes, via together ai ($10/1M tokens, $20 min) |

**citation [13]**: "DeepSeek-V3.1: Input $0.60 / Output $1.70" — [together ai cost page](https://www.together.ai/pricing)

**note:** deepseek v3.2 (73% swe-bench) is listed as "status: soon" on together ai. only v3.1 is live on serverless.

### 4.7 model #7: deepseek r1

| attribute | value |
|-----------|-------|
| model id | `deepseek-ai/DeepSeek-R1` |
| params | 671B total, 37B active (moe) |
| context | 163K tokens |
| swe-bench | — |
| input cost | $3.00 / 1M tokens |
| output cost | $7.00 / 1M tokens |
| license | mit |
| note | extended chain-of-thought reasoner |

**citation [14]**: "DeepSeek-R1: Input $3.00 / Output $7.00" — [together ai cost page](https://www.together.ai/pricing)

### 4.8 model #8: kimi k2 instruct

| attribute | value |
|-----------|-------|
| model id | `moonshotai/Kimi-K2-Instruct` |
| params | 1T total (moe) |
| context | 128K tokens |
| swe-bench | — |
| input cost | $1.00 / 1M tokens |
| output cost | $3.00 / 1M tokens |
| license | proprietary (moonshot) |

**citation [15]**: "Kimi-K2-Instruct: Input $1.00 / Output $3.00" — [together ai cost page](https://www.together.ai/pricing)

**note:** also available as `moonshotai/Kimi-K2-Instruct-0905` with 262K context.

### 4.9 model #9: llama 4 maverick

| attribute | value |
|-----------|-------|
| model id | `meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8` |
| params | 17B per expert, 128 experts (moe) |
| context | 1,048K tokens (1M) |
| swe-bench | — |
| input cost | $0.27 / 1M tokens |
| output cost | $0.85 / 1M tokens |
| license | llama 4 community |

**citation [16]**: "Llama 4 Maverick: Input $0.27 / Output $0.85" — [together ai cost page](https://www.together.ai/pricing)

**note:** 1M context window is the largest on together ai serverless. low swe-bench scores but excellent for general-purpose and long-context tasks.

### 4.10 model #10: qwen3-235b

| attribute | value |
|-----------|-------|
| model id | `Qwen/Qwen3-235B-A22B-Instruct-2507-tput` |
| params | 235B total, 22B active (moe) |
| context | 262K tokens |
| swe-bench | — |
| input cost | $0.20 / 1M tokens |
| output cost | $0.60 / 1M tokens |
| license | apache 2.0 |
| fine-tune | yes, via together ai ($6/1M tokens) |

**citation [17]**: "Qwen3-235B: Input $0.20 / Output $0.60" — [together ai cost page](https://www.together.ai/pricing)

### 4.11 notable additional models

| model id | context | input/1M | output/1M | notes |
|----------|---------|----------|-----------|-------|
| `moonshotai/Kimi-K2-Thinking` | 164K | $1.20 | $4.00 | extended thought variant of kimi k2; 71.3% swe-bench |
| `Qwen/Qwen3-235B-A22B-Thinking-2507` | 262K | $0.65 | $3.00 | extended thought variant of qwen3-235b |
| `Qwen/Qwen3-Next-80B-A3B-Instruct` | 262K | $0.15 | $1.50 | non-fp8 variant of coder-next |
| `zai-org/GLM-4.5-Air` | 128K | $0.20 | $1.10 | lighter glm variant |
| `meta-llama/Meta-Llama-3.3-70B-Instruct-Turbo` | 128K | $0.88 | $0.88 | dense 70B, no moe |
| `openai/gpt-oss-120b` | 200K | $0.15 | $0.60 | openai open-source 120B |
| `openai/gpt-oss-20b` | 200K | $0.05 | $0.20 | openai open-source 20B |

### 4.12 summary: top 10 by swe-bench score

| rank | model | swe-bench | input/1M | output/1M | context |
|------|-------|-----------|----------|-----------|---------|
| 1 | glm-5 | 77.8% | tbd | tbd | 200K |
| 2 | kimi k2.5 | 76.8% | $0.50 | $2.80 | 262K |
| 3 | qwen3-coder-next | 74.2% | $0.50 | $1.20 | 262K |
| 4 | glm-4.7 | 73.8% | $0.45 | $2.00 | 200K |
| 5 | kimi-k2 (thought variant) | 71.3% | $1.20 | $4.00 | 164K |
| 6 | qwen3-coder-480b | 69.6% | $2.00 | $2.00 | 256K |
| 7-10 | deepseek-v3.1, deepseek-r1, llama4-maverick, qwen3-235b | — | varies | varies | varies |

### 4.13 summary: top 10 by cost (cheapest input)

| rank | model | input/1M | output/1M | swe-bench |
|------|-------|----------|-----------|-----------|
| 1 | qwen3-235b | $0.20 | $0.60 | — |
| 2 | llama 4 maverick | $0.27 | $0.85 | — |
| 3 | glm-4.7 | $0.45 | $2.00 | 73.8% |
| 4 | qwen3-coder-next | $0.50 | $1.20 | 74.2% |
| 5 | kimi k2.5 | $0.50 | $2.80 | 76.8% |
| 6 | deepseek-v3.1 | $0.60 | $1.70 | — |
| 7 | kimi k2 instruct | $1.00 | $3.00 | — |
| 8 | qwen3-coder-480b | $2.00 | $2.00 | 69.6% |
| 9 | deepseek-r1 | $3.00 | $7.00 | — |
| 10 | glm-5 | tbd | tbd | 77.8% |

### 4.14 fine-tune rates

| size | lora/1M | full/1M |
|------|---------|---------|
| up to 16B | $0.48 | $0.54 |
| 17B-69B | $1.50 | $1.65 |
| 70-100B | $2.90 | $3.20 |
| frontier (480B+) | $6.00-$15.00 | — |

**citation [18]**: "Models up to 16B: LoRA $0.48 / Full $0.54" — [together ai cost page](https://www.together.ai/pricing)

### 4.15 critical correction: brief vs actual rates

the briefs cite provider-direct rates, not together ai serverless rates. key differences:

| model | brief rate (input/output) | actual together ai rate | delta |
|-------|--------------------------|-------------------------|-------|
| qwen3-coder-next | $0.07 / $0.30 | $0.50 / $1.20 | ~7x / ~4x more |
| deepseek-v3.2 | $0.28 / $0.42 | not yet available | n/a (v3.2 status: soon) |
| llama 4 maverick | $0.27 / $0.85 | $0.27 / $0.85 | matches |

**action required:** update vision document and blackbox criteria spec values to reflect actual together ai rates.

---

## 5. best practices

### 5.1 use openai sdk for compatibility

the xai adapter uses the `openai` package with base url override. together ai supports the same pattern.

**rationale:**
- same interface as xai adapter (drop-in)
- same types and error structures
- well-maintained, widely used

```typescript
import OpenAI from 'openai';

const client = new OpenAI({
  apiKey: process.env.TOGETHER_API_KEY,
  baseURL: 'https://api.together.xyz/v1',
});
```

### 5.2 structured output via json_schema

use `response_format.type = 'json_schema'` with zod-to-json-schema conversion, same as xai adapter.

```typescript
import { z } from 'zod';

const schema = z.object({ bugs: z.array(z.string()) });
const jsonSchema = z.toJSONSchema(schema);

const response = await client.chat.completions.create({
  model: 'Qwen/Qwen3-Coder-Next-80B-A3B-Instruct',
  messages: [...],
  response_format: {
    type: 'json_schema',
    json_schema: {
      name: 'response',
      strict: true,
      schema: jsonSchema,
    },
  },
});
```

### 5.3 extract token usage from response

```typescript
const tokensInput = response.usage?.prompt_tokens ?? 0;
const tokensOutput = response.usage?.completion_tokens ?? 0;
```

**note:** together ai may not provide cached token counts in the same structure as xai. verify response shape.

### 5.4 environment variable

```
TOGETHER_API_KEY=your-api-key-here
```

fail-fast if absent:

```typescript
if (!process.env.TOGETHER_API_KEY) {
  throw new Error('TOGETHER_API_KEY environment variable is required');
}
```

---

## 6. key differences from xai

| aspect | xai | together ai |
|--------|-----|-------------|
| base url | `https://api.x.ai/v1` | `https://api.together.xyz/v1` |
| env var | `XAI_API_KEY` | `TOGETHER_API_KEY` |
| model ids | `grok-code-fast-1` | `Qwen/Qwen3-Coder-Next-80B-A3B-Instruct` |
| cache tokens | `prompt_tokens_details.cached_tokens` | tbd — verify in response |
| cost range | $0.20-$15/1M | $0.20-$7/1M |

---

## sources

1. [together ai openai compatibility docs](https://docs.together.ai/docs/openai-api-compatibility)
2. [together-typescript github](https://github.com/togethercomputer/together-typescript)
3. [together-typescript github - types](https://github.com/togethercomputer/together-typescript)
4. [together ai openai compatibility docs - format](https://docs.together.ai/docs/openai-api-compatibility)
5. [together ai json mode blog](https://www.together.ai/blog/function-calling-json-mode)
6. [together-typescript github - errors](https://github.com/togethercomputer/together-typescript)
7. [together-typescript github - retries](https://github.com/togethercomputer/together-typescript)
8. [zhipu glm-5 release](https://zhipuai.cn/)
9. [together ai cost page - kimi k2.5](https://www.together.ai/pricing)
10. [together ai cost page - qwen3-coder-next](https://www.together.ai/pricing)
11. [together ai cost page - glm-4.7](https://www.together.ai/pricing)
12. [together ai cost page - qwen3-coder-480b](https://www.together.ai/pricing)
13. [together ai cost page - deepseek-v3.1](https://www.together.ai/pricing)
14. [together ai cost page - deepseek-r1](https://www.together.ai/pricing)
15. [together ai cost page - kimi-k2-instruct](https://www.together.ai/pricing)
16. [together ai cost page - llama-4-maverick](https://www.together.ai/pricing)
17. [together ai cost page - qwen3-235b](https://www.together.ai/pricing)
18. [together ai cost page - fine-tune](https://www.together.ai/pricing)
19. [together ai models page](https://www.together.ai/models)
20. [swe-bench verified leaderboard](https://www.swebench.com/)
21. [artificial analysis model benchmarks](https://artificialanalysis.ai/)
