# vision: rhachet-brains-togetherai

> a rhachet brain.atom adapter for together-ai's frontier open-source models

---

## the outcome world

### what a day-in-the-life looks like

a developer sits down to build an agentic workflow. they need a brain that can:
- reason through code
- call tools
- stay within budget

they import `rhachet-brains-togetherai`, pick a slug like `together/qwen3/coder-next`, and start:

```ts
import { genBrainAtom } from 'rhachet-brains-togetherai';

const brain = genBrainAtom({ slug: 'together/qwen3/coder-next' });

const { output } = await brain.ask({
  role: { briefs: [] },
  prompt: 'analyze this code for bugs',
  schema: { output: z.object({ bugs: z.array(z.string()) }) },
});
```

cost? **$0.07/1M input, $0.30/1M output**. that's **43x cheaper than claude sonnet 5** on input tokens.

performance? **70.6% on swe-bench verified** — near-match to grok-code-fast-1 (70.8%).

the math clicks: they can run **43x more experiments** at the same cost. or run the same experiments at **1/43rd the price**. or fine-tune the model on their domain data to close the gap further.

### before / after

| before (xai) | after (together-ai) |
|--------------|---------------------|
| grok-code-fast-1 @ $0.20/$1.50 | qwen3-coder-next @ $0.07/$0.30 |
| no fine-tune available | fine-tune @ $6/1M tokens |
| closed-source, proprietary | open-source, own your model |
| single vendor lock-in | multi-model flexibility |
| 70.8% swe-bench | 70.6% swe-bench (qwen3) / 73% (deepseek) |

### the "aha" moment

the value clicks when someone realizes:

> "wait — i can get 95% of grok's code performance at 1/5th the output cost... AND i can fine-tune it on my own data?"

that's the inflection point. open-source catches up to proprietary. fine-tune unlocks domain-specific gains. cost enables experimentation at scale.

---

## user experience

### usecases

| goal | brain choice | why |
|------|--------------|-----|
| cheap bulk code analysis | `together/qwen3/coder-next` | $0.07 input, 70.6% swe-bench |
| best open-source accuracy | `together/deepseek/v3.2` | 73% swe-bench, $0.28 input |
| max context window | `together/qwen3/coder-480b` | 262K context |
| chain-of-thought tasks | `together/kimi/k2-cot` | 71.3% swe-bench, extended thought |
| general-purpose | `together/llama4/maverick` | balanced cost/performance |
| fine-tune base | any qwen3/deepseek | $6-10/1M fine-tune cost |

### contract inputs & outputs

```ts
// choose your brain
const brain = genBrainAtom({ slug: 'together/qwen3/coder-next' });

// ask with structured output
const { output, metrics } = await brain.ask({
  role: { briefs: [roleDoc] },           // optional system context
  prompt: 'review this pr for issues',   // user prompt
  schema: { output: z.object({           // structured response
    approved: z.boolean(),
    issues: z.array(z.object({
      file: z.string(),
      line: z.number(),
      severity: z.enum(['blocker', 'nitpick']),
      message: z.string(),
    })),
  })},
});

// metrics include cost breakdown
console.log(metrics.cost.cash);
// { input: '$0.000014', output: '$0.00006', total: '$0.000074' }
```

### timelines

1. **install** (30 seconds): `npm install rhachet-brains-togetherai`
2. **configure** (1 minute): set `TOGETHER_API_KEY` env var
3. **integrate** (5 minutes): swap xai import for together-ai import
4. **validate** (10 minutes): run prior tests, observe cost savings
5. **optimize** (continuous): fine-tune on domain data, experiment with model variants

---

## mental model

### how users would describe this to a friend

> "it's like a power strip for open-source ai brains. you plug in, pick qwen3 or deepseek or llama, and get structured outputs. same interface as the xai version, but 5-10x cheaper and you can fine-tune."

### analogies

| analogy | maps to |
|---------|---------|
| **power strip** | one adapter, many brains |
| **generic drugs** | same effect, fraction of the cost |
| **linux vs windows** | open-source alternative to proprietary |
| **s3 vs proprietary storage** | commodity infrastructure |

### terms: user vs internal

| user says | we say |
|-----------|--------|
| "model" | `brain.atom` |
| "api call" | `brain.ask()` |
| "cost" | `metrics.cost.cash` |
| "context" | `spec.gain.size.context.tokens` |
| "accuracy" | `spec.gain.grades.swe` |

---

## evaluation

### how well does it solve the goals?

| goal | score | notes |
|------|-------|-------|
| cost reduction | **A+** | 5-43x cheaper than proprietary |
| performance parity | **A-** | 85-90% of proprietary (73% vs 82% swe-bench) |
| fine-tune access | **A+** | only platform with 671B+ fine-tune |
| open-source alignment | **A+** | fund the ecosystem, no vendor lock |
| drop-in replacement | **A** | same brain.atom contract, slug swap |

### pros

- **economics**: qwen3-coder-next at $0.07/$0.30 vs grok at $0.20/$1.50
- **fine-tune**: close the 10-point gap with domain-specific data
- **philosophy**: fund open-source, own your models
- **flexibility**: switch between qwen3, deepseek, llama, kimi
- **same contract**: `genBrainAtom({ slug })` works identically

### cons

- **not quite frontier**: 73% vs 82% on swe-bench (10-point gap)
- **no repl yet**: together-ai doesn't have an agentic sdk (atom only)
- **model churn**: open-source models evolve fast, need to track updates
- **rate limits**: may differ from xai's 480 req/min

### edge cases and pit of success

| edge case | how we handle it |
|-----------|------------------|
| model deprecated | slugs point to stable aliases, we update base models |
| api key absent | fail-fast with clear error: "TOGETHER_API_KEY not set" |
| unsupported schema | zod schema converts to json schema, strict mode enforced |
| token overflow | spec.gain.size.context.tokens visible, can check before call |
| cost explosion | metrics.cost.cash returned per call, visible cost tracking |

---

## available brains (proposed)

### atoms (via genBrainAtom)

| slug | model | context | swe-bench | input | output |
|------|-------|---------|-----------|-------|--------|
| `together/qwen3/coder-next` | qwen3-coder-next | 262K | 70.6% | $0.07/1M | $0.30/1M |
| `together/qwen3/coder-480b` | qwen3-coder-480b | 262K | 69.6% | $0.20/1M | $0.60/1M |
| `together/qwen3/235b` | qwen3-235b | 131K | — | $0.20/1M | $0.60/1M |
| `together/deepseek/v3.2` | deepseek-v3.2 | 128K | 73.0% | $0.28/1M | $0.42/1M |
| `together/kimi/k2` | kimi-k2 | 128K | — | $0.60/1M | $2.00/1M |
| `together/kimi/k2-cot` | kimi-k2-cot | 128K | 71.3% | $1.20/1M | $4.00/1M |
| `together/llama4/maverick` | llama-4-maverick | 128K | — | $0.30/1M | $0.60/1M |
| `together/llama3.3/70b` | llama-3.3-70b | 128K | — | $0.10/1M | $0.20/1M |
| `together/glm/4.7` | glm-4.7 | 128K | ~60% | $0.20/1M | $0.40/1M |

### future: repls (when together-ai releases agentic sdk)

together-ai currently provides inference only. when they release an agentic sdk (similar to claude code / codex), this package will add repl support.

---

## success criteria

this vision is fulfilled when:

1. **drop-in migration**: prior xai users can swap imports and run
2. **cost clarity**: every call returns visible cost breakdown
3. **spec parity**: all brain specs populated with accurate data
4. **test coverage**: integration tests prove real api calls work
5. **docs complete**: readme shows available models with prices

---

## sources

- [together ai docs](https://docs.together.ai/)
- [together ai fine-tune](https://docs.together.ai/docs/fine-tuning)
- [qwen3-coder-next benchmark](https://huggingface.co/Qwen/Qwen3-Coder-Next)
- [deepseek v3.2 benchmark](https://github.com/deepseek-ai/DeepSeek-V3)
- [swe-bench verified leaderboard](https://www.swebench.com/)
